//Cet algorithme gère le téléversement d'un fichier PDF, extrait son texte via un script Python, génère un QCM grâce à une API NLP, et renvoie les questions au client.

const express = require('express');       
const multer = require('multer');         
const axios = require('axios');           
const cors = require('cors');             
const path = require('path');             
const fs = require('fs');                 
const { exec } = require('child_process');
require('dotenv').config();

const app = express();
const upload = multer({ dest: 'uploads/' });
app.use(cors());
app.use(express.json());

// Route pour téléverser un fichier et générer un QCM
app.post('/upload', upload.single('file'), async (req, res) => {
    const chemin_fichier = req.file.path;

    exec(`python3 scripts/extract_text.py "${chemin_fichier}"`, (error, stdout) => {
        if (error) {
            return res.status(500).json({ error: 'Erreur extraction texte.' });
        }
        const texte = stdout.trim();

        genererQuestionsNLP(texte).then((questions) => {
            fs.unlinkSync(chemin_fichier); // Nettoyage
            res.json({ questions });
        });
    });
});

// Fonction pour générer les questions via API NLP
async function genererQuestionsNLP(texte) {
    const prompt = `Génère un QCM à partir de ce texte : ${texte}`;
    try {
        const response = await axios.post("https://api.ollama.com/generate", {
            prompt: prompt,
            model: "gpt-3.5-turbo"
        });
        return response.data.questions;
    } catch (error) {
        console.error("Erreur API :", error);
        return [];
    }
}

// Lancer le serveur
const PORT = process.env.PORT || 5000;
app.listen(PORT, () => {
    console.log(`Serveur backend démarré sur http://localhost:${PORT}`);
});
